{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42433a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.extend([\".\", \"..\"])  # adds the folder from which you call the script\n",
    "os.environ[\"CASAPOSE_INFERENCE\"] = \"True\"\n",
    "\n",
    "from casapose.data_handler.image_only_dataset import ImageOnlyDataset\n",
    "from casapose.data_handler.vectorfield_dataset import VectorfieldDataset\n",
    "from casapose.pose_estimation.pose_evaluation import poses_pnp\n",
    "from casapose.pose_estimation.voting_layers_2d import CoordLSVotingWeighted\n",
    "from casapose.pose_models.tfkeras import Classifiers\n",
    "from casapose.utils.config_parser import parse_config\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa7a1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = parse_config()\n",
    "\n",
    "class opt:\n",
    "    modelname = 'casapose_c_gcu5'\n",
    "    estimate_confidence = 1\n",
    "    estimate_coords = 1\n",
    "    confidence_regularization = 1\n",
    "    object = 'obj_000001,obj_000005,obj_000006,obj_000008,obj_000009,obj_000010,obj_000011,obj_000016'\n",
    "\n",
    "    no_points = 9\n",
    "    save_debug_batch = 0\n",
    "\n",
    "    imagesize = (448, 448)\n",
    "    imagesize_test = (480, 640)\n",
    "    crop_factor = 0.933333333\n",
    "    pretrained = 1\n",
    "    manualseed = 1237\n",
    "\n",
    "    # losses\n",
    "    mask_loss_weight = 1.0\n",
    "    vertex_loss_weight = 0.5\n",
    "    proxy_loss_weight = 0.015\n",
    "    keypoint_loss_weight = 0.007\n",
    "    filter_vertex_with_segmentation = 1\n",
    "    filter_high_proxy_errors = 0\n",
    "    use_bpnp_reprojection_loss = 0\n",
    "    max_keypoint_pixel_error = 12.5\n",
    "\n",
    "    # learning rate\n",
    "    lr = 0.001\n",
    "    lr_decay = 0.5\n",
    "    lr_epochs_steps = 50,75,90\n",
    "\n",
    "    # general\n",
    "    gpuids = 0,1\n",
    "    loginterval = 10\n",
    "    epochs = 100\n",
    "    batchsize = 4\n",
    "    saveinterval = 5\n",
    "    validationinterval = 1\n",
    "\n",
    "    # data preprocessing\n",
    "    workers = 0\n",
    "    prefetch = 10\n",
    "\n",
    "    # augmentation\n",
    "    translation = 0\n",
    "    rotation = 0\n",
    "    noise = 0.0001\n",
    "    brightness = 0.001\n",
    "    contrast = 0.001\n",
    "    saturation = 0.001\n",
    "    hue = 0.001\n",
    "    use_imgaug = 1\n",
    "\n",
    "    # test\n",
    "    min_object_size_test = 200\n",
    "    write_poses = 0\n",
    "    save_eval_batches = 0\n",
    "\n",
    "    # output\n",
    "    net = 'training_checkpoints'\n",
    "    outf = 'train_casapose_8_16_objects'\n",
    "\n",
    "    # config\n",
    "    train_vectors_with_ground_truth = 1\n",
    "    load_h5_weights = 0\n",
    "    copy_weights_from_backup_network = 0\n",
    "    copy_weights_add_confidence_maps = 0\n",
    "    objects_in_input_network = 8\n",
    "    objects_to_copy = 1\n",
    "    objects_to_copy_list = 'config/objects_to_copy.csv'\n",
    "    \n",
    "    confidence_filter_estimates = 1\n",
    "    confidence_choose_second = 0\n",
    "    train_vectors_with_ground_truth = 0\n",
    "    datatest_wxyz_quaterion = 0\n",
    "    filter_test_with_gt = 0\n",
    "    \n",
    "    evalf = 'output'\n",
    "    datatest = 'import_data/test/test'\n",
    "    datameshes = 'import_data/test/models'\n",
    "    #datatest = '/workspace/CASAPose/import_data/test/test'\n",
    "    #datameshes = '/workspace/CASAPose/import_data/test/models'\n",
    "    data = ''\n",
    "    datatest_path_filter = None\n",
    "    color_dataset = 1\n",
    "    train_validation_split = None #0.9\n",
    "    backbonename = 'resnet18'\n",
    "    load_h5_weights = 1\n",
    "    #load_h5_filename = '../../../data/pretrained_models/result_w'\n",
    "    load_h5_filename = 'data/pretrained_models' + '/result_w.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85931f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(opt.evalf):\n",
    "    os.makedirs(opt.evalf)\n",
    "\n",
    "checkpoint_path = opt.outf + \"/\" + opt.net\n",
    "\n",
    "frozen_path = opt.outf + \"/frozen_model\"\n",
    "img_out_path = opt.outf + \"/control_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c82d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "create_dir(img_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae769055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the manual seed.\n",
    "np.random.seed(opt.manualseed)\n",
    "tf.random.set_seed(opt.manualseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fad421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "test_dataset = None\n",
    "\n",
    "device_ids = []\n",
    "if len(opt.gpuids) == 1 and opt.gpuids[0] < 0:\n",
    "    device_ids.append(\"/cpu:0\")\n",
    "else:\n",
    "    device_ids.append(\"/gpu:{}\".format(opt.gpuids[0]))\n",
    "print(device_ids)\n",
    "\n",
    "objectsofinterest = [x.strip() for x in opt.object.split(\",\")]\n",
    "no_objects = len(objectsofinterest)\n",
    "separated_vectorfields = opt.modelname == \"pvnet\"\n",
    "\n",
    "\n",
    "testingdata = None\n",
    "normal_imgs = [0.5, 0.5]\n",
    "\n",
    "use_split = False\n",
    "if opt.data == opt.datatest:\n",
    "    print(\"split datasets with ratio {}\".format(opt.train_validation_split))\n",
    "    use_split = True\n",
    "\n",
    "test_batches = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e529b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VectorfieldDataset(\n",
    "    root=\"import_data/test/test\", # Hardcoded param\n",
    "    path_meshes=\"import_data/test/models\", # Hardcoded param\n",
    "    path_filter_root=None, # Hardcoded param\n",
    "    color_input=True, # Hardcoded param\n",
    "    no_points=9, # Taken from config_8.ini\n",
    "    objectsofinterest=objectsofinterest,\n",
    "    noise=0.00001,\n",
    "    data_size=None,\n",
    "    save=False, # Hardcoded param\n",
    "    normal=normal_imgs,\n",
    "    contrast=0.00001,\n",
    "    brightness=0.00001,\n",
    "    hue=0.00001,\n",
    "    saturation=0.00001,\n",
    "    random_translation=(0, 0),\n",
    "    random_rotation=0,\n",
    "    random_crop=False,\n",
    "    use_validation_split=use_split,\n",
    "    use_train_split=False,\n",
    "    train_validation_split=None,\n",
    "    output_folder=None,\n",
    "    visibility_filter=False,\n",
    "    separated_vectorfields=(opt.modelname == \"pvnet\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e97c7d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import_data/test/models/obj_000001/obj_000001.ply\n",
      "import_data/test/models/obj_000002/obj_000002.ply\n",
      "import_data/test/models/obj_000003/obj_000003.ply\n",
      "import_data/test/models/obj_000004/obj_000004.ply\n",
      "import_data/test/models/obj_000005/obj_000005.ply\n",
      "import_data/test/models/obj_000006/obj_000006.ply\n",
      "import_data/test/models/obj_000007/obj_000007.ply\n",
      "import_data/test/models/obj_000008/obj_000008.ply\n",
      "import_data/test/models/obj_000009/obj_000009.ply\n",
      "import_data/test/models/obj_000010/obj_000010.ply\n",
      "import_data/test/models/obj_000011/obj_000011.ply\n",
      "import_data/test/models/obj_000012/obj_000012.ply\n",
      "import_data/test/models/obj_000013/obj_000013.ply\n",
      "import_data/test/models/obj_000014/obj_000014.ply\n",
      "import_data/test/models/obj_000015/obj_000015.ply\n",
      "import_data/test/models/obj_000016/obj_000016.ply\n",
      "import_data/test/test/.DS_Store\n",
      "import_data/test/test/000001\n",
      "5\n",
      "testing data: 5.0 batches\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImageOnlyDataset(root=opt.datatest)\n",
    "testing_images, _ = test_dataset.generate_dataset(1)\n",
    "\n",
    "test_dataset = VectorfieldDataset(\n",
    "    root=opt.datatest,\n",
    "    path_meshes=opt.datameshes,\n",
    "    path_filter_root=opt.datatest_path_filter,\n",
    "    color_input=opt.color_dataset,\n",
    "    no_points=opt.no_points,\n",
    "    objectsofinterest=objectsofinterest,\n",
    "    noise=0.00001,\n",
    "    data_size=None,\n",
    "    save=opt.save_debug_batch,\n",
    "    normal=normal_imgs,\n",
    "    contrast=0.00001,\n",
    "    brightness=0.00001,\n",
    "    hue=0.00001,\n",
    "    saturation=0.00001,\n",
    "    random_translation=(0, 0),\n",
    "    random_rotation=0,\n",
    "    random_crop=False,\n",
    "    use_validation_split=use_split,\n",
    "    use_train_split=False,\n",
    "    train_validation_split=opt.train_validation_split,\n",
    "    output_folder=opt.evalf,\n",
    "    visibility_filter=False,\n",
    "    separated_vectorfields=(opt.modelname == \"pvnet\"),\n",
    ")\n",
    "print(len(test_dataset))\n",
    "testingdata, test_batches = test_dataset.generate_dataset(\n",
    "    1, 1, 0, opt.imagesize_test, 1.0, 1, no_objects, shuffle=False\n",
    ")\n",
    "\n",
    "mesh_vertex_array, mesh_vertex_count = test_dataset.generate_object_vertex_array()\n",
    "\n",
    "print(\"testing data: {} batches\".format(test_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2441298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_segmentation_shape = None\n",
    "if opt.train_vectors_with_ground_truth is True:\n",
    "    input_segmentation_shape = (\n",
    "        opt.imagesize_test[0],\n",
    "        opt.imagesize_test[1],\n",
    "        1 + no_objects,\n",
    "    )\n",
    "\n",
    "\n",
    "height = opt.imagesize_test[0]\n",
    "width = opt.imagesize_test[1]\n",
    "\n",
    "CASAPose = Classifiers.get(opt.modelname)\n",
    "ver_dim = opt.no_points * 2\n",
    "if opt.modelname == \"pvnet\":\n",
    "    ver_dim = ver_dim * no_objects\n",
    "\n",
    "if opt.estimate_confidence:\n",
    "    assert separated_vectorfields is not None, \"confidence not compaitble with this model\"\n",
    "    ver_dim += opt.no_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dd904af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<KerasTensor: shape=(None, 240, 320, 64) dtype=float32 (created by layer 'relu0')>, <KerasTensor: shape=(None, 120, 160, 64) dtype=float32 (created by layer 'stage2_unit1_relu1')>, <KerasTensor: shape=(None, 60, 80, 128) dtype=float32 (created by layer 'stage3_unit1_relu1')>, <KerasTensor: shape=(None, 60, 80, 256) dtype=float32 (created by layer 'stage4_unit1_relu1')>, <KerasTensor: shape=(None, 60, 80, 512) dtype=float32 (created by layer 'relu1')>]\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " data (InputLayer)              [(None, 480, 640, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model (Functional)             [(None, 240, 320, 6  11186889    ['data[0][0]']                   \n",
      "                                4),                                                               \n",
      "                                 (None, 120, 160, 6                                               \n",
      "                                4),                                                               \n",
      "                                 (None, 60, 80, 128                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 60, 80, 256                                               \n",
      "                                ),                                                                \n",
      "                                 (None, 60, 80, 512                                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " zero_padding2d_18 (ZeroPadding  (None, 62, 82, 512)  0          ['model[0][4]']                  \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " pv_block_1_conv2d (Conv2D)     (None, 60, 80, 256)  1179648     ['zero_padding2d_18[0][0]']      \n",
      "                                                                                                  \n",
      " pv_block_1_bn (SyncBatchNormal  (None, 60, 80, 256)  1024       ['pv_block_1_conv2d[0][0]']      \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " pv_block_1_relu (Activation)   (None, 60, 80, 256)  0           ['pv_block_1_bn[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 60, 80, 384)  0           ['pv_block_1_relu[0][0]',        \n",
      "                                                                  'model[0][2]']                  \n",
      "                                                                                                  \n",
      " zero_padding2d_19 (ZeroPadding  (None, 62, 82, 384)  0          ['concatenate[0][0]']            \n",
      " 2D)                                                                                              \n",
      "                                                                                                  \n",
      " pv_block_2_conv2d (Conv2D)     (None, 60, 80, 128)  442368      ['zero_padding2d_19[0][0]']      \n",
      "                                                                                                  \n",
      " pv_block_2_bn (SyncBatchNormal  (None, 60, 80, 128)  512        ['pv_block_2_conv2d[0][0]']      \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 60, 80, 128)  0           ['pv_block_2_bn[0][0]']          \n",
      "                                                                                                  \n",
      " pv_block_2_relu1 (Activation)  (None, 60, 80, 128)  0           ['pv_block_2_bn[0][0]']          \n",
      "                                                                                                  \n",
      " pv_block_2_relu2 (Activation)  (None, 60, 80, 128)  0           ['tf.math.multiply[0][0]']       \n",
      "                                                                                                  \n",
      " subtract (Subtract)            (None, 60, 80, 128)  0           ['pv_block_2_relu1[0][0]',       \n",
      "                                                                  'pv_block_2_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_2_upsampling (UpSampl  (None, 120, 160, 12  0          ['subtract[0][0]']               \n",
      " ing2D)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 120, 160, 19  0           ['pv_block_2_upsampling[0][0]',  \n",
      "                                2)                                'model[0][1]']                  \n",
      "                                                                                                  \n",
      " zero_padding2d_20 (ZeroPadding  (None, 122, 162, 19  0          ['concatenate_1[0][0]']          \n",
      " 2D)                            2)                                                                \n",
      "                                                                                                  \n",
      " pv_block_3_conv2d (Conv2D)     (None, 120, 160, 64  110592      ['zero_padding2d_20[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_3_bn (SyncBatchNormal  (None, 120, 160, 64  256        ['pv_block_3_conv2d[0][0]']      \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 120, 160, 64  0          ['pv_block_3_bn[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_3_relu1 (Activation)  (None, 120, 160, 64  0           ['pv_block_3_bn[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_3_relu2 (Activation)  (None, 120, 160, 64  0           ['tf.math.multiply_1[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " subtract_1 (Subtract)          (None, 120, 160, 64  0           ['pv_block_3_relu1[0][0]',       \n",
      "                                )                                 'pv_block_3_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_3_upsampling (UpSampl  (None, 240, 320, 64  0          ['subtract_1[0][0]']             \n",
      " ing2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 240, 320, 12  0           ['pv_block_3_upsampling[0][0]',  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                8)                                'model[0][0]']                  \n",
      "                                                                                                  \n",
      " zero_padding2d_21 (ZeroPadding  (None, 242, 322, 12  0          ['concatenate_2[0][0]']          \n",
      " 2D)                            8)                                                                \n",
      "                                                                                                  \n",
      " pv_block_4_conv2d (Conv2D)     (None, 240, 320, 32  36864       ['zero_padding2d_21[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_4_bn (SyncBatchNormal  (None, 240, 320, 32  128        ['pv_block_4_conv2d[0][0]']      \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 240, 320, 32  0          ['pv_block_4_bn[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_4_relu1 (Activation)  (None, 240, 320, 32  0           ['pv_block_4_bn[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_4_relu2 (Activation)  (None, 240, 320, 32  0           ['tf.math.multiply_2[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " subtract_2 (Subtract)          (None, 240, 320, 32  0           ['pv_block_4_relu1[0][0]',       \n",
      "                                )                                 'pv_block_4_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_4_upsampling (UpSampl  (None, 480, 640, 32  0          ['subtract_2[0][0]']             \n",
      " ing2D)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 480, 640, 35  0           ['pv_block_4_upsampling[0][0]',  \n",
      "                                )                                 'data[0][0]']                   \n",
      "                                                                                                  \n",
      " zero_padding2d_22 (ZeroPadding  (None, 482, 642, 35  0          ['concatenate_3[0][0]']          \n",
      " 2D)                            )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_5_conv2d (Conv2D)     (None, 480, 640, 32  10080       ['zero_padding2d_22[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_5_bn (SyncBatchNormal  (None, 480, 640, 32  128        ['pv_block_5_conv2d[0][0]']      \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 480, 640, 32  0          ['pv_block_5_bn[0][0]']          \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_5_relu1 (Activation)  (None, 480, 640, 32  0           ['pv_block_5_bn[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_5_relu2 (Activation)  (None, 480, 640, 32  0           ['tf.math.multiply_3[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " subtract_3 (Subtract)          (None, 480, 640, 32  0           ['pv_block_5_relu1[0][0]',       \n",
      "                                )                                 'pv_block_5_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_final_conv_segmentation (Co  (None, 480, 640, 9)  288        ['subtract_3[0][0]']             \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 480, 640, 9)  0          ['pv_final_conv_segmentation[0][0\n",
      " )                                                               ]']                              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 480, 640, 9)  0           ['tf.math.multiply_4[0][0]']     \n",
      "                                                                                                  \n",
      " tf.stop_gradient (TFOpLambda)  (None, 480, 640, 9)  0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " segmentation_half_size (HalfSi  (None, 240, 320, 9)  0          ['tf.stop_gradient[0][0]']       \n",
      " ze)                                                                                              \n",
      "                                                                                                  \n",
      " segmentation_quater_size (Half  (None, 120, 160, 9)  0          ['segmentation_half_size[0][0]'] \n",
      " Size)                                                                                            \n",
      "                                                                                                  \n",
      " segmentation_eighth_size (Half  (None, 60, 80, 9)   0           ['segmentation_quater_size[0][0]'\n",
      " Size)                                                           ]                                \n",
      "                                                                                                  \n",
      " pv_block_6_prepare_conv2d (Par  (None, 60, 80, 256)  1179648    ['model[0][4]',                  \n",
      " tialConvolution)                                                 'segmentation_eighth_size[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " pv_block_6_clade (ClassAdaptiv  (None, 60, 80, 256)  5120       ['pv_block_6_prepare_conv2d[0][0]\n",
      " eWeightedNormalization)                                         ',                               \n",
      "                                                                  'segmentation_eighth_size[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " pv_block_6_relu (Activation)   (None, 60, 80, 256)  0           ['pv_block_6_clade[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 60, 80, 384)  0           ['pv_block_6_relu[0][0]',        \n",
      "                                                                  'model[0][2]']                  \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pv_block_7_prepare_conv2d (Par  (None, 60, 80, 128)  442368     ['concatenate_4[0][0]',          \n",
      " tialConvolution)                                                 'segmentation_eighth_size[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " pv_block_7_clade (ClassAdaptiv  (None, 60, 80, 128)  2560       ['pv_block_7_prepare_conv2d[0][0]\n",
      " eWeightedNormalization)                                         ',                               \n",
      "                                                                  'segmentation_eighth_size[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 60, 80, 128)  0          ['pv_block_7_clade[0][0]']       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " pv_block_7_relu1 (Activation)  (None, 60, 80, 128)  0           ['pv_block_7_clade[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_7_relu2 (Activation)  (None, 60, 80, 128)  0           ['tf.math.multiply_5[0][0]']     \n",
      "                                                                                                  \n",
      " subtract_4 (Subtract)          (None, 60, 80, 128)  0           ['pv_block_7_relu1[0][0]',       \n",
      "                                                                  'pv_block_7_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_7_guided_upsamling (G  (None, 120, 160, 12  0          ['subtract_4[0][0]',             \n",
      " uidedUpsampling)               8)                                'segmentation_eighth_size[0][0]'\n",
      "                                                                 , 'segmentation_quater_size[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 120, 160, 19  0           ['pv_block_7_guided_upsamling[0][\n",
      "                                2)                               0]',                             \n",
      "                                                                  'model[0][1]']                  \n",
      "                                                                                                  \n",
      " pv_block_8_prepare_conv2d (Par  (None, 120, 160, 64  110592     ['concatenate_5[0][0]',          \n",
      " tialConvolution)               )                                 'segmentation_quater_size[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " pv_block_8_clade (ClassAdaptiv  (None, 120, 160, 64  1280       ['pv_block_8_prepare_conv2d[0][0]\n",
      " eWeightedNormalization)        )                                ',                               \n",
      "                                                                  'segmentation_quater_size[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 120, 160, 64  0          ['pv_block_8_clade[0][0]']       \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_8_relu1 (Activation)  (None, 120, 160, 64  0           ['pv_block_8_clade[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_8_relu2 (Activation)  (None, 120, 160, 64  0           ['tf.math.multiply_6[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " subtract_5 (Subtract)          (None, 120, 160, 64  0           ['pv_block_8_relu1[0][0]',       \n",
      "                                )                                 'pv_block_8_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_8_guided_upsamling (G  (None, 240, 320, 64  0          ['subtract_5[0][0]',             \n",
      " uidedUpsampling)               )                                 'segmentation_quater_size[0][0]'\n",
      "                                                                 , 'segmentation_half_size[0][0]']\n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 240, 320, 12  0           ['pv_block_8_guided_upsamling[0][\n",
      "                                8)                               0]',                             \n",
      "                                                                  'model[0][0]']                  \n",
      "                                                                                                  \n",
      " pv_block_9_prepare_conv2d (Par  (None, 240, 320, 32  36864      ['concatenate_6[0][0]',          \n",
      " tialConvolution)               )                                 'segmentation_half_size[0][0]'] \n",
      "                                                                                                  \n",
      " pv_block_9_clade (ClassAdaptiv  (None, 240, 320, 32  640        ['pv_block_9_prepare_conv2d[0][0]\n",
      " eWeightedNormalization)        )                                ',                               \n",
      "                                                                  'segmentation_half_size[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 240, 320, 32  0          ['pv_block_9_clade[0][0]']       \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_9_relu1 (Activation)  (None, 240, 320, 32  0           ['pv_block_9_clade[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_9_relu2 (Activation)  (None, 240, 320, 32  0           ['tf.math.multiply_7[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " subtract_6 (Subtract)          (None, 240, 320, 32  0           ['pv_block_9_relu1[0][0]',       \n",
      "                                )                                 'pv_block_9_relu2[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_9_guided_upsamling (G  (None, 480, 640, 32  0          ['subtract_6[0][0]',             \n",
      " uidedUpsampling)               )                                 'segmentation_half_size[0][0]', \n",
      "                                                                  'tf.stop_gradient[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 480, 640, 35  0           ['pv_block_9_guided_upsamling[0][\n",
      "                                )                                0]',                             \n",
      "                                                                  'data[0][0]']                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " pv_block_10_prepare_conv2d (Pa  (None, 480, 640, 32  10080      ['concatenate_7[0][0]',          \n",
      " rtialConvolution)              )                                 'tf.stop_gradient[0][0]']       \n",
      "                                                                                                  \n",
      " pv_block_10_clade (ClassAdapti  (None, 480, 640, 32  640        ['pv_block_10_prepare_conv2d[0][0\n",
      " veWeightedNormalization)       )                                ]',                              \n",
      "                                                                  'tf.stop_gradient[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  (None, 480, 640, 32  0          ['pv_block_10_clade[0][0]']      \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_10_relu1 (Activation)  (None, 480, 640, 32  0          ['pv_block_10_clade[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_block_10_relu2 (Activation)  (None, 480, 640, 32  0          ['tf.math.multiply_8[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " subtract_7 (Subtract)          (None, 480, 640, 32  0           ['pv_block_10_relu1[0][0]',      \n",
      "                                )                                 'pv_block_10_relu2[0][0]']      \n",
      "                                                                                                  \n",
      " pv_final_conv_vertex (Conv2D)  (None, 480, 640, 27  864         ['subtract_7[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pv_final_concatenation (Concat  (None, 480, 640, 36  0          ['pv_final_conv_segmentation[0][0\n",
      " enate)                         )                                ]',                              \n",
      "                                                                  'pv_final_conv_vertex[0][0]']   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,759,433\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,759,433\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net = CASAPose(\n",
    "    ver_dim=ver_dim,\n",
    "    seg_dim=1 + no_objects,\n",
    "    input_shape=(height, width, 3),\n",
    "    input_segmentation_shape=input_segmentation_shape,\n",
    "    weights=\"imagenet\",\n",
    "    base_model=opt.backbonename,\n",
    ")\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_path, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(network=net)  # , optimizer=optimizer)\n",
    "\n",
    "if opt.load_h5_weights is True:\n",
    "    net.load_weights(frozen_path + \"/\" + opt.load_h5_filename + \".h5\", by_name=True, skip_mismatch=True)\n",
    "\n",
    "elif opt.net != \"\":\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path)).expect_partial()\n",
    "\n",
    "for layer in net.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "11c04523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 1, 9, 3])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf4bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3a28dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runnetwork(loader_iterator, batches, no_objects):\n",
    "    \n",
    "    #@tf.function\n",
    "    def test_step(img_batch):\n",
    "        start_time = tf.timestamp()\n",
    "        \n",
    "        keypoints = img_batch[4]\n",
    "        camera_matrix = img_batch[5]\n",
    "            \n",
    "        no_points = opt.no_points\n",
    "        net_input = [tf.expand_dims(img_batch[0][0], 0)]\n",
    "        output_net = net(net_input, training=False)  # all stages are present here\n",
    "        output_seg, output_dirs, confidence = tf.split(output_net, [no_objects, no_points * 2, -1], 3)\n",
    "        coordLSV_in = [output_seg, output_dirs, confidence]\n",
    "        coords = CoordLSVotingWeighted(\n",
    "            name=\"coords_ls_voting\",\n",
    "            num_classes=no_objects,\n",
    "            num_points=no_points,\n",
    "            filter_estimates=True,\n",
    "        )(coordLSV_in)\n",
    "        poses_est = poses_pnp(\n",
    "            coords, output_seg, keypoints, camera_matrix, no_objects - 1, min_num=opt.min_object_size_test\n",
    "        )\n",
    "        tf.print(poses_est[0, 0, 0, 0, 0])  # access memory to get sure everything is done\n",
    "\n",
    "        end_time = tf.timestamp()\n",
    "        time_needed = end_time - start_time\n",
    "        return time_needed, poses_est, coords\n",
    "\n",
    "    speed = []\n",
    "    img_batches = []\n",
    "    est_poses = []\n",
    "    coords_list = []\n",
    "    \n",
    "    for batch_idx in range(batches):\n",
    "        img_batch = loader_iterator.get_next()\n",
    "                \n",
    "        time_needed, poses_est, coords = test_step(img_batch)\n",
    "        speed.append(time_needed)\n",
    "        with open(opt.evalf + \"/speed_eval.csv\", \"a\") as file:\n",
    "            s = \"{},{:.7f}\\n\".format(batch_idx + 1, time_needed)\n",
    "            file.write(s)\n",
    "            \n",
    "        img_batches.append(img_batch)\n",
    "        est_poses.append(poses_est)\n",
    "        coords_list.append(coords)\n",
    "\n",
    "    tf.print(\"average speed: {}\".format(tf.reduce_mean(speed[10:])), summarize=-1)\n",
    "\n",
    "    return loader_iterator, img_batches, est_poses, coords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "7c36245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batches: 5.0 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Batches: {} \".format(test_batches))\n",
    "\n",
    "testingdata_iterator = iter(testingdata)\n",
    "img_batch = testingdata_iterator.get_next()\n",
    "\n",
    "keypoints = img_batch[4]\n",
    "camera_matrix = img_batch[5]\n",
    "\n",
    "type(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "7506a7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.397043586\n"
     ]
    }
   ],
   "source": [
    "#testingdata_iterator = iter(testing_images)\n",
    "\n",
    "testingdata_iterator = iter(testingdata)\n",
    "\n",
    "loader_iterator, img_batches, est_poses, coords_list = runnetwork(testingdata_iterator, \n",
    "                                                                  int(test_batches),\n",
    "                                                                  no_objects + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e0ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8586bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4001b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from casapose.utils.image_utils import get_all_vectorfields\n",
    "from casapose.utils.loss_functions import (\n",
    "    keypoint_reprojection_loss,\n",
    "    proxy_voting_dist,\n",
    "    proxy_voting_loss_v2,\n",
    "    smooth_l1_loss,\n",
    ")\n",
    "from casapose.pose_estimation.pose_evaluation import (\n",
    "    estimate_and_evaluate_poses,\n",
    "    evaluate_pose_estimates,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_loss(\n",
    "    output_seg,\n",
    "    target_seg,\n",
    "    output_vertex,\n",
    "    target_vertex,\n",
    "    target_points,\n",
    "    mask_loss_weight=1.0,\n",
    "    vertex_loss_weight=1.0,\n",
    "    proxy_loss_weight=1.0,\n",
    "    kp_loss_weight=0.0,\n",
    "    kp_loss=None,\n",
    "):\n",
    "\n",
    "    oc = np.int32(target_seg.shape[3] - 1)  # object count\n",
    "    vc = target_points.shape[3] * 2  # vertex count\n",
    "    kp_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    mask_loss = tf.reduce_mean(\n",
    "        input_tensor=tf.nn.softmax_cross_entropy_with_logits(labels=target_seg, logits=output_seg)\n",
    "    )\n",
    "    separated_vectors = oc > 1 and output_vertex.shape[-1] == (oc * vc)  # original pvnet with multiple objects\n",
    "\n",
    "    if kp_loss is None:\n",
    "        kp_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "    if separated_vectors:  #\n",
    "        vertex_loss = sum(\n",
    "            smooth_l1_loss(\n",
    "                output_vertex[:, :, :, i * vc : (i + 1) * vc],\n",
    "                target_vertex[:, :, :, i * vc : (i + 1) * vc],\n",
    "                target_seg[:, :, :, i + 1 : i + 2],\n",
    "            )\n",
    "            for i in range(oc)\n",
    "        )\n",
    "        proxy_loss = sum(\n",
    "            proxy_voting_loss_v2(\n",
    "                output_vertex[:, :, :, i * vc : (i + 1) * vc],\n",
    "                target_points[:, i : i + 1, :, :, :],\n",
    "                vertex_one_hot_weights=target_seg[:, :, :, i + 1 : i + 2],\n",
    "                vertex_weights=target_seg[:, :, :, i + 1 : i + 2],\n",
    "            )\n",
    "            for i in range(oc)\n",
    "        )\n",
    "    else:\n",
    "        vertex_loss = smooth_l1_loss(\n",
    "            output_vertex,\n",
    "            target_vertex,\n",
    "            target_seg[:, :, :, 0:1],\n",
    "            invert_weights=True,\n",
    "        )\n",
    "        proxy_loss = proxy_voting_loss_v2(\n",
    "            output_vertex,\n",
    "            target_points,\n",
    "            vertex_one_hot_weights=target_seg[:, :, :, 1:],\n",
    "            vertex_weights=target_seg[:, :, :, 0:1],\n",
    "            invert_weights=True,\n",
    "            loss_per_object=False,\n",
    "        )\n",
    "\n",
    "    loss = (\n",
    "        tf.multiply(mask_loss, mask_loss_weight)\n",
    "        + tf.multiply(proxy_loss, proxy_loss_weight)\n",
    "        + tf.multiply(vertex_loss, vertex_loss_weight)\n",
    "        + tf.multiply(kp_loss, kp_loss_weight)\n",
    "    )\n",
    "    return [loss, mask_loss, vertex_loss, proxy_loss, kp_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7740bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from casapose.utils.inf_dataset_utils import save_eval_batch\n",
    "\n",
    "def runnetwork(loader_iterator, batches):\n",
    "    \n",
    "    images = []\n",
    "    stats = []\n",
    "    est_poses = []\n",
    "    image_batches = []\n",
    "    \n",
    "    for batch_idx in range(batches):\n",
    "\n",
    "        img_batch = loader_iterator.get_next()\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        target_seg = img_batch[1]\n",
    "        target_vertex = img_batch[3]\n",
    "        keypoints = img_batch[4]\n",
    "        cam_mat = img_batch[5]\n",
    "        diameters = img_batch[6]\n",
    "        offsets = img_batch[7]\n",
    "        filtered_seg = img_batch[8]\n",
    "        poses_gt = img_batch[10]\n",
    "        confidence = None\n",
    "        kp_loss = None\n",
    "        no_features = target_vertex.shape[3]  # vertex count\n",
    "        no_points = opt.no_points\n",
    "        no_objects = target_seg.shape[3]\n",
    "\n",
    "        separated_vectorfields = opt.modelname == \"pvnet\"\n",
    "\n",
    "        target_dirs = get_all_vectorfields(\n",
    "            target_seg,\n",
    "            target_vertex,\n",
    "            filtered_seg,\n",
    "            separated_vectorfields,\n",
    "        )\n",
    "\n",
    "        net_input = [img_batch[0]]\n",
    "        output_net = net(net_input, training=False)  # all stages are present here\n",
    "\n",
    "        if opt.estimate_confidence:\n",
    "            output_seg, output_dirs, confidence = tf.split(output_net, [no_objects, no_points * 2, -1], 3)\n",
    "        else:\n",
    "            output_seg, output_dirs = tf.split(output_net, [no_objects, -1], 3)\n",
    "\n",
    "        if opt.estimate_coords:\n",
    "            if opt.train_vectors_with_ground_truth:\n",
    "                coordLSV_in = [target_seg, output_dirs, confidence]\n",
    "            else:\n",
    "                coordLSV_in = [output_seg, output_dirs, confidence]\n",
    "\n",
    "            coords = CoordLSVotingWeighted(\n",
    "                name=\"coords_ls_voting\",\n",
    "                num_classes=no_objects,\n",
    "                num_points=no_points,\n",
    "                filter_estimates=opt.confidence_filter_estimates,\n",
    "                output_second_largest_component=opt.confidence_choose_second,\n",
    "            )(coordLSV_in)\n",
    "\n",
    "            kp_loss, poses_est, points_est = keypoint_reprojection_loss(\n",
    "                coords, \n",
    "                output_seg,\n",
    "                poses_gt,\n",
    "                keypoints,\n",
    "                target_seg,\n",
    "                cam_mat,\n",
    "                offsets,\n",
    "                confidence,\n",
    "                min_num=opt.min_object_size_test,\n",
    "                min_num_gt=1,\n",
    "                use_bpnp_reprojection_loss=opt.use_bpnp_reprojection_loss,\n",
    "                estimate_poses=True,\n",
    "                filter_with_gt=opt.filter_test_with_gt,\n",
    "            )\n",
    "\n",
    "        if opt.estimate_coords:\n",
    "            pose_stats, estimated_poses, estimated_points = evaluate_pose_estimates(\n",
    "                points_est,\n",
    "                poses_est,\n",
    "                poses_gt,\n",
    "                target_seg,\n",
    "                keypoints,\n",
    "                cam_mat,\n",
    "                diameters,\n",
    "                evaluation_points=mesh_vertex_array,\n",
    "                object_points_3d_count=mesh_vertex_count,\n",
    "                min_num=1,\n",
    "            )\n",
    "            estimated_poses = tf.squeeze(estimated_poses, axis=2)\n",
    "        else:\n",
    "            pose_stats, estimated_poses, estimated_points = estimate_and_evaluate_poses(\n",
    "                output_seg,\n",
    "                target_seg,\n",
    "                output_dirs,\n",
    "                poses_gt,\n",
    "                keypoints,\n",
    "                cam_mat,\n",
    "                diameters,\n",
    "                offsets,\n",
    "                evaluation_points=mesh_vertex_array,\n",
    "                object_points_3d_count=mesh_vertex_count,\n",
    "                min_num=1,\n",
    "            )\n",
    "\n",
    "        # end_time = tf.timestamp()\n",
    "        # time_needed = end_time - start_time\n",
    "\n",
    "        loss = compute_loss(\n",
    "            output_seg,\n",
    "            target_seg,\n",
    "            output_dirs,\n",
    "            target_dirs,\n",
    "            target_vertex,\n",
    "            opt.mask_loss_weight,\n",
    "            opt.vertex_loss_weight,\n",
    "            opt.proxy_loss_weight,\n",
    "            opt.keypoint_loss_weight,\n",
    "            kp_loss=kp_loss,\n",
    "        )\n",
    "        loss.append(pose_stats)\n",
    "\n",
    "        _, object_loss_values = proxy_voting_dist(\n",
    "            output_dirs,\n",
    "            target_vertex,\n",
    "            vertex_one_hot_weights=target_seg[:, :, :, 1:],\n",
    "            vertex_weights=target_seg[:, :, :, 0:1],\n",
    "            invert_weights=True,\n",
    "        )\n",
    "        loss.append(object_loss_values)\n",
    "\n",
    "        beta = tf.cast(1e6, dtype=output_seg.dtype)\n",
    "        hot_seg = tf.expand_dims(tf.expand_dims(tf.nn.softmax(output_seg * beta), -1), -1)[:, :, :, 1:, :, :]\n",
    "        w = tf.math.softplus(confidence)\n",
    "        hot_seg_ = tf.squeeze(hot_seg, axis=-1)\n",
    "        w_ = hot_seg_ * tf.expand_dims(w, axis=-2)\n",
    "        w_max = tf.reduce_max(w_, axis=[0, 1, 2], keepdims=True)\n",
    "        w_min = tf.reduce_min(w_, axis=[0, 1, 2], keepdims=True)\n",
    "        w_ = tf.math.divide_no_nan(tf.subtract(w_, w_min), tf.subtract(w_max, w_min))\n",
    "        confidence = tf.reduce_sum(w_, axis=-2)\n",
    "\n",
    "        add_correct = loss[5][1]\n",
    "        \n",
    "        image = save_eval_batch(\n",
    "                img_batch,\n",
    "                output_seg,\n",
    "                target_dirs,\n",
    "                output_dirs,\n",
    "                estimated_poses,\n",
    "                estimated_points,\n",
    "                no_objects - 1,\n",
    "                no_features,\n",
    "                # path_out=opt.evalf + \"/visual_batch_eval_mask\",\n",
    "                confidence=confidence,\n",
    "                add_correct=add_correct,\n",
    "                )\n",
    "            \n",
    "        #tf.print(\"Estimated Coordinates: {}\".format(estimated_points), summarize=-1)\n",
    "        #tf.print(\"Estimated Poses: {}\".format(estimated_poses), summarize=-1)\n",
    "        #tf.print(\"Segmentation Masks: {}\".format(output_seg), summarize=-1)\n",
    "        #tf.print(\"Image: {}\".format(type(image)), summarize=-1)\n",
    "        #tf.print(\"Labels: TESTING\") #{}\".format(test_pose_count_gt), summarize=-1)\n",
    "        tf.print(\"Objects of Interest: {}\".format(objectsofinterest), summarize=-1)\n",
    "        tf.print(\"length: {}\".format(len(objectsofinterest)), summarize=-1)\n",
    "        tf.print(\"Number of Objects: {}\".format(no_objects), summarize=-1)\n",
    "        \n",
    "\n",
    "        images.append(image)\n",
    "        stats.append(pose_stats)\n",
    "        est_poses.append(estimated_poses)\n",
    "        image_batches.append(img_batch)\n",
    "    \n",
    "    return images, stats, est_poses, image_batches\n",
    "        \n",
    "\n",
    "        #return image, pose_stats, estimated_poses, output_seg, loss\n",
    "        # return loss, estimated_points, estimated_poses, output_seg\n",
    "        # return estimated_points, estimated_poses, output_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "820cea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batches: 5.0 \n",
      "[TensorShape([8, 9, 2]), TensorShape([8, 9, 3]), TensorShape([3, 3])]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "Objects of Interest: ['obj_000001', 'obj_000005', 'obj_000006', 'obj_000008', 'obj_000009', 'obj_000010', 'obj_000011', 'obj_000016']\n",
      "length: 8\n",
      "Number of Objects: 9\n",
      "[TensorShape([8, 9, 2]), TensorShape([8, 9, 3]), TensorShape([3, 3])]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "Objects of Interest: ['obj_000001', 'obj_000005', 'obj_000006', 'obj_000008', 'obj_000009', 'obj_000010', 'obj_000011', 'obj_000016']\n",
      "length: 8\n",
      "Number of Objects: 9\n",
      "[TensorShape([8, 9, 2]), TensorShape([8, 9, 3]), TensorShape([3, 3])]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "Objects of Interest: ['obj_000001', 'obj_000005', 'obj_000006', 'obj_000008', 'obj_000009', 'obj_000010', 'obj_000011', 'obj_000016']\n",
      "length: 8\n",
      "Number of Objects: 9\n",
      "[TensorShape([8, 9, 2]), TensorShape([8, 9, 3]), TensorShape([3, 3])]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "Objects of Interest: ['obj_000001', 'obj_000005', 'obj_000006', 'obj_000008', 'obj_000009', 'obj_000010', 'obj_000011', 'obj_000016']\n",
      "length: 8\n",
      "Number of Objects: 9\n",
      "[TensorShape([8, 9, 2]), TensorShape([8, 9, 3]), TensorShape([3, 3])]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "Objects of Interest: ['obj_000001', 'obj_000005', 'obj_000006', 'obj_000008', 'obj_000009', 'obj_000010', 'obj_000011', 'obj_000016']\n",
      "length: 8\n",
      "Number of Objects: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Batches: {} \".format(test_batches))\n",
    "\n",
    "testingdata_iterator = iter(testingdata)\n",
    "images, stats, est_poses, image_batches = runnetwork(testingdata_iterator, int(test_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e18d7afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 3, 4])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_poses[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc3434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e920af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b1ddbd51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dc82a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from casapose.utils.geometry_utils import apply_offsets, project\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "from casapose.utils.draw_utils import draw_bb\n",
    "\n",
    "import cv2\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "def save_poses_single_sample(\n",
    "    img,\n",
    "    estimated_poses,\n",
    "    gt_pose,\n",
    "    offsets,\n",
    "    cuboids,\n",
    "    keypoints,\n",
    "    camera_matrix,\n",
    "    path,\n",
    "    file_prefix,\n",
    "    normal=[0.5, 0.5],\n",
    "):\n",
    "        \n",
    "    img_keypoints = tf.cast(((img * normal[1]) + normal[0]) * 255, dtype=tf.uint8).numpy()\n",
    "    img_cuboids = img_keypoints.copy()\n",
    "\n",
    "    eps = 1e-4\n",
    "    \n",
    "    for obj_idx, obj_pose in enumerate(estimated_poses):\n",
    "        \n",
    "        print(obj_idx)\n",
    "\n",
    "        inst_idx = 0\n",
    "        obj_pose_est = estimated_poses[obj_idx].numpy()\n",
    "        instance_cuboids = cuboids[obj_idx][inst_idx].numpy()\n",
    "        instance_keypoints = keypoints[obj_idx][inst_idx].numpy()     \n",
    "\n",
    "\n",
    "        valid_est = np.abs(np.sum(obj_pose_est)) > eps\n",
    "        # draw bb - ESTIMATED\n",
    "        if valid_est: \n",
    "            print('est')\n",
    "            print(obj_pose_est.shape)\n",
    "            transformed_cuboid_points2d, _ = project(instance_cuboids, camera_matrix.numpy(), obj_pose_est)   \n",
    "            \n",
    "            #transformed_cuboid_points2d = apply_offsets(transformed_cuboid_points2d, offsets)\n",
    "            #transformed_cuboid_points2d = transformed_cuboid_points2d.numpy()\n",
    "            #transformed_cuboid_points2d = np.reshape(transformed_cuboid_points2d, (8,2))\n",
    "            \n",
    "            print(transformed_cuboid_points2d.shape)\n",
    "            draw_bb(transformed_cuboid_points2d, img_cuboids, (0, 255, 0))\n",
    "        else: \n",
    "            print('skipped obj est')\n",
    "\n",
    "        # GT\n",
    "        if gt_pose is not None: \n",
    "            instance_pose_gt = gt_poses[obj_idx][inst_idx].numpy()\n",
    "            valid_gt = np.abs(np.sum(instance_pose_gt)) > eps\n",
    "            \n",
    "            if valid_gt: \n",
    "                print('gt')\n",
    "                gt_pose_est = gt_pose[obj_idx].numpy()\n",
    "                print(gt_pose_est[0].shape)\n",
    "                transformed_cuboid_points2d_gt, _ = project(instance_cuboids, \n",
    "                                                            camera_matrix.numpy(), \n",
    "                                                            gt_pose_est[0])   \n",
    "                print(gt_pose_est[0])\n",
    "                draw_bb(transformed_cuboid_points2d_gt, img_cuboids, (255, 0, 0))\n",
    "            else: \n",
    "                print('skipped obj')\n",
    "\n",
    "    # save image\n",
    "    img_cuboids = Image.fromarray((img_cuboids).astype(\"uint8\"))\n",
    "    img_cuboids.save(path + \"/\" + str(file_prefix) + \"_cuboids_all.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c978fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata_iterator = iter(testingdata)\n",
    "img_batch = testingdata_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b02b2e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 1, 3, 4])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#img_batch = img_batches[0]\n",
    "poses_est = est_poses[0]\n",
    "poses_est.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "807767a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 3, 4])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_est = np.reshape(poses_est, (1, 8, 3, 4))\n",
    "poses_est.shape\n",
    "# needs to be: \n",
    "# TensorShape([1, 8, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297eaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "4c675293",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img_batch[0][0]\n",
    "keypoints = img_batch[4][0]\n",
    "cam_mat = img_batch[5][0]\n",
    "offsets = img_batch[7][0]\n",
    "cuboids = img_batch[9][0]\n",
    "gt_poses = img_batch[10][0]\n",
    "estimated_poses=poses_est[0]\n",
    "path_out = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9c6f0449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 1, 3, 4])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_poses.shape\n",
    "# TensorShape([8, 1, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d08b3a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "est\n",
      "(3, 4)\n",
      "(8, 2)\n",
      "gt\n",
      "(3, 4)\n",
      "[[-8.3105749e-01 -4.9617833e-01  2.5129774e-01  1.8149728e+02]\n",
      " [-5.1251614e-01  5.0768352e-01 -6.9252050e-01  3.1408548e+02]\n",
      " [ 2.1603394e-01 -7.0431846e-01 -6.7621356e-01  1.0016328e+03]]\n",
      "1\n",
      "skipped obj est\n",
      "gt\n",
      "(3, 4)\n",
      "[[ 9.4615239e-01  2.0407134e-01  2.5129774e-01 -6.1099270e+01]\n",
      " [ 3.2305470e-01 -6.4502031e-01 -6.9252038e-01  3.7482285e+02]\n",
      " [ 2.0768568e-02  7.3641270e-01 -6.7621368e-01  7.7388177e+02]]\n",
      "2\n",
      "est\n",
      "(3, 4)\n",
      "(8, 2)\n",
      "gt\n",
      "(3, 4)\n",
      "[[ 5.2214855e-01 -8.1499100e-01  2.5129774e-01  2.2954282e+02]\n",
      " [-4.9126270e-01 -5.2827692e-01 -6.9252038e-01  7.4394020e+01]\n",
      " [ 6.9715267e-01  2.3814531e-01 -6.7621374e-01  1.2459657e+03]]\n",
      "3\n",
      "skipped obj est\n",
      "gt\n",
      "(3, 4)\n",
      "[[-4.1060883e-01 -8.7649864e-01  2.5129774e-01  1.7613786e+02]\n",
      " [-7.0892721e-01  1.3355748e-01 -6.9252044e-01  1.8576587e+02]\n",
      " [ 5.7343054e-01 -4.6250683e-01 -6.7621362e-01  1.0451112e+03]]\n",
      "4\n",
      "est\n",
      "(3, 4)\n",
      "(8, 2)\n",
      "gt\n",
      "(3, 4)\n",
      "[[ 2.2433765e-01 -9.4155300e-01  2.5129780e-01  2.7690518e+02]\n",
      " [-6.3793588e-01 -3.3682850e-01 -6.9252032e-01  2.3907364e+02]\n",
      " [ 7.3668879e-01 -4.9535246e-03 -6.7621374e-01  1.1183993e+03]]\n",
      "5\n",
      "est\n",
      "(3, 4)\n",
      "(8, 2)\n",
      "gt\n",
      "(3, 4)\n",
      "[[-3.0575556e-01  9.1834795e-01  2.5129777e-01 -2.7712762e+02]\n",
      " [ 6.0606241e-01  3.9128497e-01 -6.9252032e-01 -2.7123423e+01]\n",
      " [-7.3430365e-01 -5.9439793e-02 -6.7621374e-01  1.1972905e+03]]\n",
      "6\n",
      "est\n",
      "(3, 4)\n",
      "(8, 2)\n",
      "gt\n",
      "(3, 4)\n",
      "[[ 7.8057885e-01  5.7231647e-01  2.5129774e-01  4.6566129e-05]\n",
      " [ 5.5809546e-01 -4.5710492e-01 -6.9252044e-01  4.0978193e-05]\n",
      " [-2.8147143e-01  6.8081492e-01 -6.7621362e-01  1.1959166e+03]]\n",
      "7\n",
      "est\n",
      "(3, 4)\n",
      "(8, 2)\n",
      "gt\n",
      "(3, 4)\n",
      "[[ 7.23870754e-01  6.42542303e-01  2.51297712e-01 -1.25131325e+02]\n",
      " [ 5.98250031e-01 -4.03128147e-01 -6.92520499e-01  7.35345612e+01]\n",
      " [-3.43668550e-01  6.51634157e-01 -6.76213622e-01  1.15425024e+03]]\n"
     ]
    }
   ],
   "source": [
    "save_poses_single_sample(\n",
    "    img = img,\n",
    "    estimated_poses=estimated_poses,\n",
    "    gt_pose=gt_poses,\n",
    "    offsets=offsets,\n",
    "    cuboids=cuboids,\n",
    "    keypoints=keypoints,\n",
    "    camera_matrix=cam_mat,\n",
    "    path=path_out,\n",
    "    file_prefix=0,\n",
    "    normal=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21628ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ffe7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
